{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"REG_EQN_Aubeuf.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"dclc19H7w-Lo"},"source":["# <center>Machine Learning</center>\n","\n","## La régression linéaire via l'équation normale\n","### Clément AUBEUF"]},{"cell_type":"markdown","metadata":{"id":"W8bUZK70w-Lq"},"source":["----\n","\n","<a\n","  target=\"_blank\" href=\"https://colab.research.google.com/drive/12W7jPdXUI9Nv8_JaynT8cVVN-QXkaViG\"> \n","  <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> \n","  Ouvrir dans Google Colab\n","</a>\n","\n","----"]},{"cell_type":"markdown","metadata":{"id":"zEIi44wjw-Lr"},"source":["### Importation de bibliothèques & Initialisation de nos valeurs"]},{"cell_type":"code","metadata":{"id":"fE39zkuxw-Ls"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_regression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gu2siiutw-Lx"},"source":["x, y = make_regression(n_samples=50, n_features=1, noise=5) # on génère des données aléatoires grâce à make_regression()\n","plt.scatter(x, y) # et on affiche les données"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"25AKER9Mw-L3"},"source":["### Avec Numpy"]},{"cell_type":"code","metadata":{"id":"oyaL_uQ9w-L4"},"source":["b = np.linalg.inv(x.T.dot(x)).dot(x.T).dot(y)\n","# On calcule theta via la formule theta = (X^{T}*X)^{-1}*X^{T}*Y\n","\n","# puis on définit notre y calculé \n","y_calcule = x.dot(b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4oZNaCTw-L8"},"source":["plt.scatter(x,y)\n","\n","# On peut maintenant tracer la droite correspondant à f(x)=ax+b avec nos y calculés\n","plt.plot(x,y_calcule, color=\"green\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BiEamMwJw-L_"},"source":["### Avec SKLearn"]},{"cell_type":"code","metadata":{"id":"WAGP_jpnw-MA"},"source":["from sklearn.linear_model import LinearRegression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFza8JXUw-MD"},"source":["model = LinearRegression() # On choisit le type de modèle : régression linéaire\n","model.fit(x,y) # On construit notre modèle prédictif\n","y_calcule_ = model.predict(x) # On applique notre modèle prédictif sur les données d'entrée de base (sur x)\n","\n","plt.scatter(x, y)\n","plt.plot(x, y_calcule_, c='r')\n","plt.grid()\n","\n","print('Précision du modèle : ', model.score(x,y)*100, '%') \n","# model.score prédit une valeur y' grâce au modèle construit, puis le compare avec y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ilj6fzAuw-MH"},"source":["print('coef:', model.coef_) # On récupère notre a\n","print('intercept:', model.intercept_) # et notre b\n","\n","a = model.coef_\n","b = model.intercept_\n","\n","# a est sous forme de tableau car la regréssion linéaire peut être appliquée à une fonction polynomiale \n","# de type f(x) = a0x0 + a1x1 + a2x2 + ... + b\n","# -> donc avec plusieurs résultats possibles pour a (d'où l'utilité du tableau)\n","\n","# Dans notre cas, l'équation est de type ax + b, donc avec un seul a\n","\n","# On arrondit les résultats obtenus\n","a = round(a[0],2) \n","b = round(b,2)\n","\n","# Et on affiche la fonction obtenue\n","print()\n","print('f(x) =', a, 'x +', b)"],"execution_count":null,"outputs":[]}]}